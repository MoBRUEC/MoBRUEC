[
  { "id": "NG1", "cat": "governance", "q": "Which body acts as the steering committee that establishes standards and holds the highest level of authority for approving data policies?", "options": ["Data Governance Board (or Council)", "Data Governance Office (DGO)", "Data Stewards Council", "IT Executive Steering Committee", "Enterprise Architecture Review Board"], "a": 0, "expl": "The Data Governance Board acts as the steering committee that establishes standards and ensures enterprise-wide compliance.", "prep": "DMBOK2 Ch. 3: Data Governance Board = highest authority for policies.", "ref": "DMBOK2 Ch. 3" },
  { "id": "NG2", "cat": "governance", "q": "Who handles the daily administration of the data governance program and ensures that policies are communicated and followed throughout the organization?", "options": ["Data Governance Officer (DGO)", "Chief Data Officer (CDO)", "Lead Business Data Steward", "Executive Data Sponsor", "Corporate Compliance Officer"], "a": 0, "expl": "The Data Governance Officer handles the daily administration of the program.", "prep": "DMBOK2 Ch. 3: DGO = daily administration.", "ref": "DMBOK2 Ch. 3" },
  { "id": "NG3", "cat": "governance", "q": "Which role represents business executives who have the ultimate accountability for a specific data domain?", "options": ["Data Owners", "Business Data Stewards", "Technical Data Custodians", "Data Governance Council Members", "Chief Information Officers (CIOs)"], "a": 0, "expl": "Data Owners are business executives who have the ultimate accountability for a specific data domain.", "prep": "DMBOK2 Ch. 3: Owners = Accountability.", "ref": "DMBOK2 Ch. 3" },
  { "id": "NG4", "cat": "governance", "q": "Who is specifically tasked with ensuring the accuracy and integrity of data and must maintain Data Dictionaries including a full change history?", "options": ["Data Stewards", "Data Owners", "Database Administrators (DBAs)", "Enterprise Data Architects", "Data Quality Analysts"], "a": 0, "expl": "Data Stewards are responsible for the day-to-day management of data quality and metadata within their specific business areas.", "prep": "DMBOK2 Ch. 3: Stewards = day-to-day management.", "ref": "DMBOK2 Ch. 3" },
  { "id": "NG5", "cat": "governance", "q": "What often has a more direct impact on an organization than individual Data Stewards themselves because it automates the application of policy?", "options": ["A dynamic data classification system", "A centralized master data management (MDM) hub", "An enterprise data warehouse (EDW)", "A commercial data catalog software tool", "A real-time data quality dashboard"], "a": 0, "expl": "A dynamic data classification system automates the application of policy, having a direct impact.", "prep": "DMBOK2 Ch. 3: Dynamic classification = automated policy.", "ref": "DMBOK2 Ch. 3" },
  { "id": "NG6", "cat": "governance", "q": "Which metric is the prioritized metric for demonstrating the effectiveness of a Governance program?", "options": ["A percentage increase in data quality scores", "The total number of data stewards appointed across the enterprise", "The total volume of data assets successfully cataloged", "The measurable reduction in database storage and processing costs", "The number of data policies written and formally approved"], "a": 0, "expl": "A percentage increase in data quality scores is the prioritized metric for demonstrating effectiveness.", "prep": "DMBOK2 Ch. 3: Metric = DQ score increase.", "ref": "DMBOK2 Ch. 3" },
  { "id": "NG7", "cat": "governance", "q": "The presence of documented policies indicates an organization is at which level of maturity (e.g., in the CMMI-based DAMA maturity model)?", "options": ["Defined (Level 3)", "Managed (Level 2)", "Optimizing (Level 5)", "Initial (Level 1)", "Predictable / Quantitatively Managed (Level 4)"], "a": 0, "expl": "Documented policies indicate the Defined level (Level 3).", "prep": "DMBOK2 Ch. 15: Defined = Documented policies.", "ref": "DMBOK2 Ch. 15" },
  { "id": "NG8", "cat": "governance", "q": "Which maturity state is characterized by success depending on specific projects or heroic individual efforts, with processes being inconsistent across the broader organization?", "options": ["Managed (Level 2)", "Initial (Level 1)", "Defined (Level 3)", "Predictable (Level 4)", "Optimizing (Level 5)"], "a": 0, "expl": "Managed is the state when success depends on specific projects and processes are inconsistent.", "prep": "DMBOK2 Ch. 15: Managed = Project-dependent success.", "ref": "DMBOK2 Ch. 15" },
  { "id": "NG9", "cat": "governance", "q": "What provides the human accountability required for any successful data governance framework?", "options": ["Stewardship and Ownership roles", "Automated Data Quality Management tools", "Centralized Enterprise Metadata Repositories", "A robust Enterprise Data Architecture", "A fully implemented Master Data Management system"], "a": 0, "expl": "Stewardship and Ownership provide the human accountability required for any governance framework.", "prep": "DMBOK2 Ch. 3: Human accountability = Stewardship & Ownership.", "ref": "DMBOK2 Ch. 3" },
  { "id": "NME1", "cat": "metadata", "q": "What is the primary goal of Metadata Management?", "options": ["Ensuring that data is accurately defined, understood, and usable", "Ensuring that all enterprise data is stored securely and encrypted", "Ensuring that data is integrated in real-time across all systems", "Ensuring that historical data is archived properly to reduce costs", "Ensuring that all physical data models are fully normalized to 3NF"], "a": 0, "expl": "The goal is ensuring that data is accurately defined. Metadata provides the essential context and meaning.", "prep": "DMBOK2 Ch. 12: Goal = accurately defined data.", "ref": "DMBOK2 Ch. 12" },
  { "id": "NME2", "cat": "metadata", "q": "Which international standard is specifically designed for metadata registries?", "options": ["ISO/IEC 11179", "ISO 8000 (Data Quality)", "ISO/IEC 27001 (Information Security)", "ISO 9001 (Quality Management)", "TOGAF (The Open Group Architecture Framework)"], "a": 0, "expl": "ISO/IEC 11179 is the standard for metadata registries.", "prep": "DMBOK2 Ch. 12: ISO 11179 = Metadata.", "ref": "DMBOK2 Ch. 12" },
  { "id": "NME3", "cat": "metadata", "q": "In metadata management, what describes the specific conditions or circumstances under which a value domain is applicable?", "options": ["Context", "Taxonomy", "Ontology", "Data Lineage", "Data Format"], "a": 0, "expl": "Context describes the specific conditions or circumstances under which a value domain is applicable.", "prep": "DMBOK2 Ch. 12: Context = conditions for value domain.", "ref": "DMBOK2 Ch. 12" },
  { "id": "NME4", "cat": "metadata", "q": "What are Data Catalog tools primarily used for?", "options": ["To inventory, manage, and discover data assets across the enterprise", "To design and generate physical database schemas (DDL)", "To perform real-time data integration and ETL processing", "To encrypt sensitive data at rest within the database", "To automatically execute data quality cleansing and standardization rules"], "a": 0, "expl": "Data Catalog tools are primarily used to manage and discover data assets across the enterprise.", "prep": "DMBOK2 Ch. 12: Data Catalog = discover assets.", "ref": "DMBOK2 Ch. 12" },
  { "id": "NS1", "cat": "security", "q": "In the event of a suspected Data Breach, what is the mandatory first action according to incident response best practices?", "options": ["Perform a Forensic Analysis to preserve evidence and understand the scope", "Immediately notify all internal stakeholders and the Board of Directors", "Immediately notify all external customers and the public media", "Shut down all enterprise servers to prevent further data exfiltration", "Restore all compromised data from the most recent secure backups"], "a": 0, "expl": "The mandatory first action is to perform a Forensic Analysis to ensure the source is understood and evidence is preserved.", "prep": "DMBOK2 Ch. 7: Breach response = Forensic Analysis first.", "ref": "DMBOK2 Ch. 7" },
  { "id": "NS2", "cat": "security", "q": "Which technique is prioritized for GDPR compliance when providing data to third parties while maintaining the ability to re-identify it later if legally required?", "options": ["Pseudonymization", "Anonymization", "Dynamic Data Masking", "Encryption at rest", "Tokenization"], "a": 0, "expl": "Pseudonymization replaces private identifiers with artificial ones, keeping the re-identification key separate.", "prep": "DMBOK2 Ch. 7: Pseudonymization = re-identification possible.", "ref": "DMBOK2 Ch. 7" },
  { "id": "NMD1", "cat": "mdm", "q": "Which MDM implementation style is the most lightweight and flexible approach, involving a central index of business keys while leaving the actual descriptive data in the source systems?", "options": ["Registry pattern", "Consolidated pattern", "Coexistence pattern", "Transaction (or Centralized) pattern", "Federated pattern"], "a": 0, "expl": "The Registry pattern is the most lightweight and flexible approach for MDM.", "prep": "DMBOK2 Ch. 10: Registry = lightweight index.", "ref": "DMBOK2 Ch. 10" },
  { "id": "NMD2", "cat": "mdm", "q": "For an MDM consolidation initiative, what is the most crucial first step before selecting a tool or defining rules?", "options": ["Map out the current data landscape and identify existing data silos", "Purchase an enterprise-grade MDM software solution", "Define the golden record survivorship and merge rules", "Migrate all existing master data to a cloud data warehouse", "Establish a formal data governance council and appoint data owners"], "a": 0, "expl": "The most crucial first step is to map out the current data landscape and existing data silos.", "prep": "DMBOK2 Ch. 10: Consolidation first step = map landscape.", "ref": "DMBOK2 Ch. 10" },
  { "id": "NMD3", "cat": "mdm", "q": "Which of the following is a recommended technical practice for performance tuning initial MDM data loads?", "options": ["Use bulk data load utilities and disable database indexes during the load process", "Use row-by-row inserts (RBAR) to ensure strict referential data integrity", "Enable all database indexes and constraints during the load process", "Schedule massive data loads during peak business usage hours", "Increase database logging levels to maximum to capture all load errors"], "a": 0, "expl": "Performance tuning requires bulk loads, disabling indexes, parallel processing, and adjusting logging levels.", "prep": "DMBOK2 Ch. 10: MDM tuning = bulk load, disable indexes.", "ref": "DMBOK2 Ch. 10" },
  { "id": "NIQ1", "cat": "integration", "q": "What is a key component of data integration governance that ensures data flowing between systems meets required standards before being committed to the target?", "options": ["Data validation rules applied during the ETL/ELT process", "Data encryption keys managed by a central certificate authority", "Data cataloging and metadata tagging of the source systems", "Dynamic data masking applied at the presentation layer", "Data archiving policies applied to the source systems"], "a": 0, "expl": "Data validation rules ensure data meets standards before it is committed to a target database.", "prep": "DMBOK2 Ch. 8: Validation rules = integration governance.", "ref": "DMBOK2 Ch. 8" },
  { "id": "NIQ2", "cat": "quality", "q": "Which data profiling technique is the primary method for gaining insights into missing values, nullability, and data distribution patterns of individual fields?", "options": ["Column Profiling", "Cross-table (or Relationship) Profiling", "Data Cleansing and Standardization", "Entity Resolution and Matching", "Data Parsing and Formatting"], "a": 0, "expl": "Column Profiling focuses on the characteristics of individual fields.", "prep": "DMBOK2 Ch. 13: Column Profiling = individual fields.", "ref": "DMBOK2 Ch. 13" }
]
